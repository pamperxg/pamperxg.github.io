---
title: ML基础算法概念
date: 2018-09-22 20:42:56
tags: [notes,DataScience,机器学习]
---

**统计学习**偏向于理论的优美完善。**机器学习**是统计学习对实践技术的延伸，为实际问题提供算法支撑。**数据挖掘**偏重于大量数据的实践应用，注重实际问题的解决。

<!--more-->

**WTF**：从给定的训练数据集合出发，**假设**数据是独立同分布产生的，**假设**要学习的模型是某个函数集合（假设空间），有某评价准则，从假设空间挑选最优模型。

其中涉及的变换空间有：输入空间、特征空间、输出空间

包含三个要素：**模型**、**策略**、**算法**

---

##### 基本概念

- 监督学习关于数据的基本假设：输入输出的随机变量遵循联合概率分布（P(X,Y)）。

  分为概率模型和非概率模型，分别表示为条件概率分布（P(Y|X)）和决策函数（Y=f(X)）。

- 损失函数和风险函数：一次预测的好坏和平均意义下模型预测结果的好坏

- 风险函数：期望损失： 损失函数*联合概率分布的二重积分

*由于联合概率是未知的，根据大数定律，当样本容量趋于无穷时，经验风险趋于期望风险，所以可以有经验风险来求期望风险，但是由于样本的数量有限，效果往往达不到预期。*

- 监督学习的两个基本策略：经验风险最小化(empiracal risk minimization)和结构风险最小化(structural risk minimization)
- ERM：样本容量足够大的时候能达到较好的效果，样本容量较小的时候容易产生过拟合，例：极大似然估计（模型是条件概率分布，损失函数是对数损失，经验风险最小化等价于极大似然估计）。
- SRM：通过正则化系数来权衡经验风险和模型复杂度，模型越复杂时，正则项越大，例：最大后验估计（模型是条件概率分布，损失函数为对数损失函数，模型复杂度由先验概率表示，结构风险最小化等价于最大后验估计）。

正则项：

- 当经验风险项较小时，表示模型较复杂，包含的参数越多，正则项就会越大。
- 正则项的作用就是选择经验风险和模型复杂度同时较小的模型。奥卡姆剃刀法则（能很好地解释已知数据并且十分简单的才是最好的模型）。
- 从贝叶斯的角度看，正则项相当于模型的先验概率，简单的模型有较大的先验概率，复杂的模型有较小的先验概率。
- 可以看成是一种模型选择的手段，其他模型选择方法还有：交叉验证（重复使用数据）。

生成模型和判别模型

- 生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测模型。

- 判别方法直接学习决策函数或条件概率分布。

- 生成方法可以还原出联合概率分布，学习收敛速度快，样本容量的增加时可以更快收敛于真实模型，存在隐变量时可以使用而不能使用判别模型。

- 判别模型直接面对预测，往往学习的准确率更高，可以对数据进行各种程度的抽象，定义特征，简化学习问题。

判别数据集是否线性可分

- 低维直接画图判断，高维检查凸包是否相交。

---

##### Naive贝叶斯：

- 贝叶斯定理+特征条件独立性假设。
- 对于给定的数据集，基于特征条件独立假设学习输入输出的联合概率分布，然后基于此模型，对于给定的输入X，利用贝叶斯定理求出后验概率（P(Y|X)）。（联合概率分布 == 先验概率(P(Y))*条件概率(P(X|Y))（条件独立性假设））
- 学习到生成数据的机制，即生成模型。
- 分类的特征在类确定的条件下都是条件独立的。
- 后验概率最大化 == 0-1损失时的期望风险最小化。

---

##### 决策树：

*互斥且完备的if-then规则集合，定义在特征空间和类空间上的条件概率分布，模型可读性强，分类速度快。学习过程是由训练数据预估条件概率模型（从训练数据中归纳出一组分类规则）。*

- 该算法主要包含三个步骤：特征选择、决策树的生成、剪枝
- 内部结点表示一个特征或者属性，叶子节点表示一个类。
- **损失函数**：正则化的极大似然函数，**策略**：以损失函数为目标的最小化。

*从所有可能的决策树中选取最优决策树是NP完全问题，所以决策树的学习通常采用启发式算法，近似求解这一最优化问题，得到一个次最优的决策树。*

- 过程：递归选择最优特征，根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程。

  这种方法容易过拟合，所以要注意剪枝过程。

- 如果特征的数量众多，在开始学习之前，可以对特征进行选择，留下对数据具有足够分类能力的特征。

- 树的生成只考虑局部最优，剪枝考虑全局最优。

**熵**：不确定性的大小。

- 当各个微观状态出现概率相等时，熵达到最大值。

  熵值依赖于X的分布，与其大小无关。

- 条件熵：X给定的条件下，条件概率分布的熵对X的数学期望。（按某一个特征分类后的和）

- 经验熵，经验条件熵：由数据估计得到的熵和条件熵。

- 信息增益：经验熵与给定特征条件下经验条件熵之差。

*熵与条件熵之差称为互信息，决策树中的信息增益等价于训练集中类与特征的互信息。*

**特征选择**：

信息增益（ID3），信息增益比（C4.5）（信息增益存在偏向于选择取值较多的特征的问题，信息增益比可以对这个问题进行校正）。

**树的生成**：

启发式算法，每一步最优分割。

**剪枝**：通过极小化整体的损失函数或者代价函数实现，正则项限制复杂度。

分为预剪枝（一边建立一边剪枝，控制树深度、叶子节点样本数、叶子节点个数、信息增益量）和后剪枝（优化损失函数）

决策树的生成只考虑了信息增益（比）对训练集更好地拟合，而剪枝考虑减小模型的复杂度，利用损失函数最小化原则进行剪枝就是用正则化的极大似然估计进行模型选择。

计算每个节点的经验熵，递归地从叶子节点往上回缩，直到选得损失函数最小的子树。

**CART**：（二叉树）

在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。

**生成树**：

递归构建二叉树，对回归树用**平方误差最小化**准则，分类树用**基尼指数最小化**准则进行特征选择，生成二叉树。

**剪枝**：

从生成算法的决策树底端开始不断剪枝，直到根节点，形成一个子树序列，然后通过交叉验证在独立的验证数据集上对子树序列进行测试，选取最优子树。











