---
title: ML&DL基础概念
date: 2018-08-25 20:42:56
tags: [notes,DataScience,MachineLearning,DeepLearning]
---

**统计学习**偏向于理论的优美完善，研究泛化误差界。**机器学习**是统计学习对实践技术的延伸，为实际问题提供算法支撑。**数据挖掘**偏重于大量数据的实践应用，注重实际问题的解决。

<!--more-->

### ML

**WTF**：从给定的训练数据集合出发，**假设**数据是独立同分布产生的，**假设**要学习的模型是某个函数集合（假设空间），有某评价准则，从假设空间挑选最优模型。

其中涉及的变换空间有：输入空间、特征空间、输出空间

统计学习三个要素：**模型**、**策略**、**算法**

目标：研究泛化误差界

---

##### 基本概念

- 监督学习关于数据的基本假设：输入输出的随机变量遵循联合概率分布（$P(X,Y)$）。

  分为概率模型和非概率模型，分别表示为条件概率分布（$P(Y|X)$）和决策函数（$Y=f(X)$）。

- 损失函数和风险函数：一次预测的好坏和平均意义下模型预测结果的好坏

- 风险函数：期望损失： 损失函数*联合概率分布的二重积分

*由于联合概率是未知的，根据大数定律，当样本容量趋于无穷时，经验风险趋于期望风险，所以可以由经验风险来求期望风险，但是由于样本的数量有限，效果往往达不到预期。*

- 监督学习的两个基本策略：经验风险最小化(empiracal risk minimization)和结构风险最小化(structural risk minimization)
- ERM：样本容量足够大的时候能达到较好的效果，样本容量较小的时候容易产生过拟合，例：极大似然估计（模型是条件概率分布，损失函数是对数损失，经验风险最小化等价于极大似然估计）。
- SRM：通过正则化系数来权衡经验风险和模型复杂度，模型越复杂时，正则项越大，例：最大后验估计（模型是条件概率分布，损失函数为对数损失函数，模型复杂度由先验概率表示，结构风险最小化等价于最大后验估计）。

贝叶斯定理中任意一个量都可以从联合概率分布得到，要么通过积分的方式，要么通过关于某个合适的变量求条件概率。

频率派和贝叶斯派区别和联系：

- 频率观点：通过最优化某些准则（如似然函数）来确定参数的具体值。
- 贝叶斯观点：给定观察数据，我们引入参数的先验分布，然后使用贝叶斯定理来计算对应的后验概率。

共轭先验：使得后验概率分布的函数形式与先验概率相同，一般称先验分布与似然函数是共轭的。

熵、互信息？

KL散度与极大似然？

过拟合和欠拟合：

欠拟合指的是模型不能在训练集上获得足够低的误差，过拟合指的是训练误差和测试误差的差距太大。

调整模型的**容量**，

当算法容量适合任务的复杂度和所提供数据的数量时，算法效果最佳。

VC维：

> 量化模型（二元分类器）容量，该分类器能够分类的训练样本的最大数目。训练误差和泛化误差之间差异的上界随着模型容量的增长而增长，但是随着训练样本增多而下降。

No Free Lunch theorem：

> 在所有可能的数据生成分布上平均后，每一个分类算法在未事先观测的点上都有相同的错误率。没有一个机器学习算法总是比其他的要好。但是这个结论只在我们考虑所有的数据生成分布时才成立。真实应用中，如果我们对遇到的概率分布进行假设，则可以设计在这些分布上效果良好的学习算法。
>
> 机器学习是理解什么样的分布于人工智能获取经验的“真实世界”相关，以及什么样的学习算法在我们关注的数据生成分布上效果要好。

正则项：

> 修改学习算法，使其降低泛化误差而非训练误差

- 当经验风险项较小时，表示模型较复杂，包含的参数越多，正则项就会越大。
- 正则项的作用就是选择经验风险和模型复杂度同时较小的模型。奥卡姆剃刀法则（能很好地解释已知数据并且十分简单的才是最好的模型）。
- 从贝叶斯的角度看，正则项相当于模型的先验概率，简单的模型有较大的先验概率，复杂的模型有较小的先验概率。
- 可以看成是一种模型选择的手段，其他模型选择方法还有：交叉验证（重复使用数据）。

在模型中增加更多的特征一般会增加训练样本的准确率，减小bias。但是测试样本的准确率不一定增加，除非增加的特征为有效特征。

生成模型和判别模型

- 生成方法由数据学习联合概率分布，然后求出条件概率分布作为预测模型。

- 判别方法直接学习决策函数或条件概率分布。

- 生成方法可以还原出联合概率分布，学习收敛速度快，样本容量的增加时可以更快收敛于真实模型，存在隐变量时可以使用而不能使用判别模型。

- 判别模型直接面对预测，往往学习的准确率更高，可以对数据进行各种程度的抽象，定义特征，简化学习问题。

判别数据集是否线性可分

- 低维直接画图判断，高维检查凸包是否相交。

模型选择

- prml第一章中，多项式曲线拟合主要在于1、**函数空间**的选择（拟合三角函数为何用多项式函数），其中正则化方法对函数空间进行限制，使得函数空间变小。2、确定了多项式拟合后，**阶数**如何选择。
- 一个模型中可能存在多个控制模型复杂度的参数。
- 最大似然的方法容易过拟合，尤其是数据集小的时候（太小不服从大数定律？频率派的思想？）
- 在数据量充足的情况下，一般选择在验证集上表现好的模型。但是在验证集很小的情况下对预测的表现估计会有一定的噪声。如果模型的设计使用有限规模的数据集迭代很多次，对于验证集会发生一定程度的过拟合。因此，保留一个第三方的测试集很有必要，最终使用该数据集来进行模型选择。
- 在训练集和测试集有限的情况下，可以使用交叉验证（CV、Loocv）来充分利用数据。但是也有其缺点，交叉验证需要进行训练的次数随着折数的增加而增加，如果训练本身很耗时不太适用。对于一个模型有多个复杂度参数的情况，探索这些参数的组合所需的训练次数是参数个数的指数函数（$2^n$）。
- 模型选择的理想情况：模型的选择依赖于训练数据，且允许在一轮训练中对比多个超参数和模型类型。
- 信息准则：赤池信息准则（AIC【考虑模型的复杂度和模型的拟合度】）、贝叶斯信息准则（BIC）（倾向于选择过于简单的模型）。。

泛函，指函数张成的空间。例Taylor公式，任意一个函数可以放到（$1,x,x^2,x^3...$）所张成的函数空间，如果是有限个基的话就称为欧式空间，无穷的话就是Hilbert空间。

回归分析，p-value(假设检验的角度)？

维度灾难

（特征的数量有一个邻界点，过多带来过拟合）

- 如果把空间的区域分割成一个一个的单元格，单元格的数量随着空间的维数以指数的形式增大。此时，为了保证这些单元格都不为空，需要指数级增长的训练数据（维度在指数级地爆炸增长）。

  > 增加维度来获得最佳线性分类效果等价于在低维空间中使用非线性模型

- 对于一个M阶多项式，系数数量的增长速度类似于D（维度）的M次方。$y(x,w)=w_0+\sum_{i=1}^{D}w_ix_i+\sum_{i=1}^{D}\sum_{j=1}^{D}w_{ij}x_ix_j+\sum_{i=1}^{D}\sum_{j=1}^{D}\sum_{k=1}^{D}w_{ijk}x_ix_jx_k$

- D很大时，一个球体的大部分体积都聚集在表面附近的薄壳上（样本密度越来越小，数据的稀疏性变得愈发明显）。

- 距离度量（欧式距离）失效，最近距离和最远距离变得不可辨别

各种分布

> 【随着数据量的增加，参数的后验分布等于最大似然解。在数据量足够的情况下,先验知识就显得不是那么重要了。】

- 伯努利分布（二项分布的特例）

  二元随机变量$x\in\{0,1\}$，概率分布：$p(x=1|\mu)=\mu$，$Beru(x|\mu)=\mu^x(1-\mu)^{(1-x)}$，$E[x]=\mu,var[x]=\mu(1-\mu)$，对于一个观测数据集$D=\{x_1,...,x_n\}$，似然函数为：$P(D|\mu)=\prod_{i=1}^{N}p(x_n|\mu)$

- 二项式分布（泊松分布是二项式分布的极限形式，二项式分布也能推出正态分布）

- beta分布（引入先验，二项分布的共轭分布。beta分布于二项式分布的似然函数有着相同的形式，当用beta分布作为二项式分布参数的先验分布，乘似然函数后得到的后验分布依然是beta分布）

- 多项式分布-->狄利克雷分布（二项->抛硬币，多项->掷骰子）

- 高斯分布

- t分布（无限个均值一样，方差不同的高斯分布混合而成）

- 混合高斯模型（隐变量：数据点属于哪个高斯分布，EM（隐变量，模型参数））

- 指数族

---

##### Naive贝叶斯：

- 贝叶斯定理+特征条件独立性假设。
- 对于给定的数据集，基于特征条件独立假设学习输入输出的联合概率分布($P(X,Y)​$)，然后基于此模型，对于给定的输入$X​$，利用贝叶斯定理求出后验概率($P(Y|X)​$)。（联合概率分布 == 先验概率($P(Y)​$)*条件概率($P(X|Y)​$)（条件独立性假设））
- 学习到生成数据的机制，即生成模型。
- 分类的特征在类确定的条件下都是条件独立的。
- 后验概率最大化 == 0-1损失时的期望风险最小化。

---

##### 决策树：

*互斥且完备的if-then规则集合，定义在特征空间和类空间上的条件概率分布，模型可读性强，分类速度快。学习过程是由训练数据预估条件概率模型（从训练数据中归纳出一组分类规则）。*

- 该算法主要包含三个步骤：特征选择、决策树的生成、剪枝
- 内部结点表示一个特征或者属性，叶子节点表示一个类。
- **损失函数**：正则化的极大似然函数，**策略**：以损失函数为目标的最小化。

*从所有可能的决策树中选取最优决策树是NP完全问题，所以决策树的学习通常采用启发式算法，近似求解这一最优化问题，得到一个次最优的决策树。*

- 过程：递归选择最优特征，根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程。

  这种方法容易过拟合，所以要注意剪枝过程。

- 如果特征的数量众多，在开始学习之前，可以对特征进行选择，留下对数据具有足够分类能力的特征。

- 树的生成只考虑局部最优，剪枝考虑全局最优。

**熵**：不确定性的大小。

- 当各个微观状态出现概率相等时，熵达到最大值。

  熵值依赖于X的分布，与其大小无关。

- 条件熵：X给定的条件下，条件概率分布的熵对X的数学期望。（按某一个特征分类后的和）

- 经验熵，经验条件熵：由数据估计得到的熵和条件熵。

- 信息增益：经验熵与给定特征条件下经验条件熵之差。

*熵与条件熵之差称为互信息，决策树中的信息增益等价于训练集中类与特征的互信息。*

**特征选择**：

信息增益（ID3），信息增益比（C4.5）（信息增益存在偏向于选择取值较多的特征的问题，信息增益比可以对这个问题进行校正）。

**树的生成**：

启发式算法，每一步最优分割。

**剪枝**：通过极小化整体的损失函数或者代价函数实现，正则项限制复杂度。

分为预剪枝（一边建立一边剪枝，控制树深度、叶子节点样本数、叶子节点个数、信息增益量）和后剪枝（优化损失函数）

决策树的生成只考虑了信息增益（比）对训练集更好地拟合，而剪枝考虑减小模型的复杂度，利用损失函数最小化原则进行剪枝就是用正则化的极大似然估计进行模型选择。

计算每个节点的经验熵，递归地从叶子节点往上回缩，直到选得损失函数最小的子树。

**CART**：（二叉树）

在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。

**生成树**：

递归构建二叉树，对回归树用**平方误差最小化**准则，分类树用**基尼指数最小化**准则进行特征选择，生成二叉树。

**剪枝**：

从生成算法的决策树底端开始不断剪枝，直到根节点，形成一个子树序列，然后通过交叉验证在独立的验证数据集上对子树序列进行测试，选取最优子树。

---

> 涉及到**距离度量**时，缺失的数据就会比较重要。

**随机森林和XGBoost缺失值处理**：

- 随机森林：1、数值型变量用所有对应类别中的中位数替换，类别型变量用出现最多的值（众数）替换

  ​		   2、加权，相似的点拥有更高的权重		  

- XGBoost：训练时缺失数据会被分到左子树或者右子树计算损失，选择较优的那个。如果训练时没有缺失而预测时出现缺失，则默认被分到右子树。

---

**PCA**：

> 求解协方差矩阵的topK大的特征向量

1、数据在低维线性空间（主子空间）上的正交投影，使得投影数据的方差最大化

2、使得投影代价（数据点和它们的投影间的平方距离）最小的线性投影

标准化有什么好处？

> 防止过分捕捉某些数值大的特征，让每个维度的重要性一样。
>
> 有利于梯度下降法的收敛。

---

**Kmeans**:

> k均值算法对应于用于高斯混合模型的EM算法的一个特定的非概率极限
>
> k-means算法是高斯混合聚类在混合成分方差相等，且每个样本仅指派一个混合成分时候的特例。 

数据集$\{x_n\}$，聚类中心$\{\mu_k\}$，定义一个目标函数：
$$
J=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}||x_n-\mu_k||^2
$$
如果数据点$x_n$被分配到类别$k$，则$r_{nk}=1$

目标是寻找$r_{nk}$和$\mu_k$的值，使得$J$最小化，可以使用一种迭代的方法完成这件事，每次迭代涉及两个连续步骤，分别为$r_{nk}$的最优化和$\mu_k$的最优化。

- 选择$\mu_k$的初始值

- 关于$r_{nk}$优化$J$，保持$\mu_k$固定（E）

  $r_{nk}= \begin{cases}1&\mbox{if $k=arg\ min_j ||x_n-\mu_j||^2$}, \\0 & \text{else}.\end{cases}$

- 关于$\mu_k$优化$J$，保持$r_{nk}$固定（M）

  $\partial J/\partial \mu_k = 2\sum_{n=1}^{N}r_{nk}(x_n-\mu_k)=0$

  $\Rightarrow \mu_k=\frac{\sum_nr_{nk}x_n}{\sum_nr_{nk}}$

- 不断重复这两个阶段直至收敛

**高斯混合模型**：

![GMM](GMM.png)

---

**线性模型**：

GLM建模过程：先明确y服从什么分布，找到一个合适的链接函数，将参数$\theta$映射成整条实直线，规定$f(\theta) = a + bx$ 。[推导](https://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247487577&amp;idx=3&amp;sn=eaa836d0bf6aeb4e454a73f42f70340b&source=41#wechat_redirect)

回归的线性模型和分类的线性模型：基于**固定**非线性基函数的线性组合。$y(x,w)=f(\sum_{j=1}^{M}w_j\phi_j(x))$,$f(.)$在分类问题中是一个非线性激活函数，在回归问题中为恒等函数。

线性回归的模型可以表达非线性的东西，因为基函数是非线性的（这里基函数是固定的，NN、SVM是变化基）。

多项式基函数的局限：它们是输入变量的全局函数，对于输入空间一个区域的改变将会影响所有的其他区域。

对$x$做非线性变化，再对这些基函数线性组合，会使得计算量变得庞大，可以使用kernel trick，直接由核函数来计算内积，来解决非线性关系。

固定基函数的线性组合构成的回归模型和分类模型，它们的实际应用被**维数灾难**问题限制了。为了将这些模型应用于大规模问题，有必要根据数据调节基函数。（SVM、神经网络）

如何避免维度灾难问题？

kernel function 满足交换律（内积满足交换律），满足Mercer定理（核矩阵是半正定的）。

> 有一些模式识别方法，对于新输入的预测纯粹依靠学习到的参数向量$w$，也有一类方法，训练数据点或者它的一个子集在预测阶段任然保留并且被使用。如最近邻方法（memory-based，训练速度快，预测速度慢）、核函数方法（每一个核函数都以训练数据为中心）。

核方法：

许多线性参数模型可以被转化为一个等价的“对偶表示”，预测的基础也是在训练数据点处计算的核函数的线性组合。对于基于固定非线性特征空间映射$\phi(x)$的模型来说，核函数关系如下：
$$
k(x,x^\prime) = \phi(x)^T\phi(x^\prime)
$$
对偶表示：完全通过核函数表示，可以直接针对核函数进行计算，**避免了显式地引入特征向量**$\phi(x)$，这使得我们可以隐式地使用高维特征空间，甚至无限维特征空间。

构造核：核函数对应于某个特征空间的标量积，合法核函数的充分必要条件：Gram矩阵在所有的集合$\{x_n\}$的选择下都是半正定的。

多项式核

高斯核？无穷维？

sigmoid核（Gram矩阵是非半正定的也可以使用）

很多基于核的学习算法最大的局限是核函数必须对所有的可能点求值，这在训练阶段的计算上是不可行的。

$\Longrightarrow\Downarrow$

SVM(分类，回归，异常检测)

线性可分：存在$w,b$,有$t_ny_n > 0$

margin：决策边界与任意样本之间的最小距离，这个决策边界的位置由数据点的一个子集确定，这些数据点称为支持向量

soft margin：引入松弛变量，允许一些训练数据点被错分

核函数对应于特征空间的内积，特征空间可以是高维甚至是无穷维的，通过直接对核函数操作而不显式引入特征空间，可以有效地避免维度灾难的问题。（然而。。）

对新输入的预测只通过支持向量来完成，但是训练阶段使用了整个数据集，所以一个解决二次规划的高效算法很重要。

SMO：

多分类SVM：

SVR：

---

### DL

##### 基本概念：

**神经网络**

通用近似原理：一个带有线性输出的两层网络可以在任意精度下近似任何输入变量较少的连续函数，只要隐含单元的数量足够多。

前馈神经网络（MLP）：使用参数形式的基函数，这些参数可以在训练阶段调节。

三层网络函数：$i(D维),j(M维)[a=w^Tx,z=h(a)],k(K维)[a = w^Tz,\sigma(a)]$
$$
y_k(x,w) = \sigma(\sum_{j=0}^{M}w_{kj}^{(2)}h(\sum_{i=0}^{D}w_{ji}^{(1)}x_i)
$$
误差方向传播：提供了计算导数的一个高效方法  
$$
\delta_j = h^\prime(a_j)\sum_k(\delta_kw_{kj})
$$
![BP](BP.jpg)

卷积网络：

将不变性的性质融入到神经网络结构中，以构造对输入变量的变换具有不变性的模型，并能提取多个局部特征（读个feature map）

特点：1、卷积（不变性，检测不同位置的相同模式） 2、权值共享（feature map） 3、下采样（微小平移不敏感）

**梯度下降法的步骤**：

（梯度下降法、批量最优化方法[共轭梯度法、拟牛顿法]、随机梯度下降（高效地处理数据中的冗余性，可以逃离局部最小值点））

1、随机初始化权重和偏差

2、把输入传入网络得到输出值

3、计算预测值与真实值之间的误差

4、对每一个产生误差的神经元，调整相应的权重值以减小误差

5、重复迭代，直至网络权重达到最佳值

**梯度消失爆炸**：



**各类激活函数比较**：















